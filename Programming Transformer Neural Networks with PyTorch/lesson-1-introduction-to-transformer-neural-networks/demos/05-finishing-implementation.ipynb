{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CMWMOaDg-ZBv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Use CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "text = Path('tiny-shakespeare.txt').read_text(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IwwOe-tJ-xcE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    THE SONNETS\n",
      "    ALL’S WELL THAT ENDS WELL\n",
      "    THE TRAGEDY OF ANTONY AND CLEOPATRA\n",
      "    AS YOU LIKE IT\n",
      "    THE COMEDY OF ERRORS\n",
      "    THE TRAGEDY OF CORIOLANUS\n",
      "    CYMBELINE\n",
      "    THE TRAGEDY OF HAMLET, PRINCE OF DENMARK\n",
      "    THE FIRST PART OF KING HENRY THE FOURTH\n",
      "    THE SECOND PART OF KING HENRY THE FOURTH\n",
      "    THE LIFE OF KING HENRY THE FIFTH\n",
      "    THE FIRST PART OF HENRY THE SIXTH\n",
      "    THE SECOND PART OF KING HENRY THE SIXTH\n",
      "    THE THIRD PART OF KING HENRY THE SIXTH\n",
      "    KING HENRY THE EIGHTH\n",
      "    THE LIFE AND DEATH OF KING JOHN\n",
      "    THE TRAGEDY OF JULIUS CAESAR\n",
      "    THE TRAGEDY OF KING LEAR\n",
      "    LOVE’S LABOUR’S LOST\n",
      "    THE TRAGEDY OF MACBETH\n",
      "    MEASURE FOR MEASURE\n",
      "    THE MERCHANT OF VENICE\n",
      "    THE MERRY WIVES OF WINDSOR\n",
      "    A MIDSUMMER NIGHT’S DREAM\n",
      "    MUCH ADO ABOUT NOTHING\n",
      "    THE TRAGEDY OF OTHELLO, THE MOOR OF VENICE\n",
      "    PERICLES, PRINCE OF TYRE\n",
      "    KING RICHARD THE SECOND\n",
      "    KING RICHARD THE THIRD\n",
      "    THE TRAGEDY OF ROMEO AND JULIET\n",
      "    THE TAMING OF THE SHREW\n",
      "    THE TEMPEST\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(text[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ap_Ixr0M-0Yv"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CharTokenizer:\n",
    "  def __init__(self, vocabulary):\n",
    "    self.token_id_for_char = {char: token_id for token_id, char in enumerate(vocabulary)}\n",
    "    self.char_for_token_id = {token_id: char for token_id, char in enumerate(vocabulary)}\n",
    "\n",
    "  @staticmethod\n",
    "  def train_from_text(text):\n",
    "    vocabulary = set(text)\n",
    "    return CharTokenizer(sorted(list(vocabulary)))\n",
    "\n",
    "  def encode(self, text):\n",
    "    token_ids = []\n",
    "    for char in text:\n",
    "      token_ids.append(self.token_id_for_char[char])\n",
    "    return torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "  def decode(self, token_ids):\n",
    "    chars = []\n",
    "    for token_id in token_ids.tolist():\n",
    "      chars.append(self.char_for_token_id[token_id])\n",
    "    return ''.join(chars)\n",
    "\n",
    "\n",
    "  def vocabulary_size(self):\n",
    "    return len(self.token_id_for_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "T3q9Dj3l-2Ja"
   },
   "outputs": [],
   "source": [
    "tokenizer = CharTokenizer.train_from_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Lb1zImZr-4mY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([31, 57, 64, 64, 67,  2, 75, 67, 70, 64, 56])\n",
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"Hello world\"))\n",
    "print(tokenizer.decode(tokenizer.encode(\"Hello world\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MlTEiIqs-7Uz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 98\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size: {tokenizer.vocabulary_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7Qal76ig-94U"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TokenIdsDataset(Dataset):\n",
    "  def __init__(self, data, block_size):\n",
    "    self.data = data\n",
    "    self.block_size = block_size\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data) - self.block_size\n",
    "\n",
    "  def __getitem__(self, pos):\n",
    "    assert pos < len(self.data) - self.block_size\n",
    "\n",
    "    x = self.data[pos:pos + self.block_size]\n",
    "    y = self.data[pos + 1:pos + 1 + self.block_size]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"vocabulary_size\": tokenizer.vocabulary_size(),\n",
    "  \"context_size\": 256,\n",
    "  \"embedding_dim\": 768,\n",
    "  \"heads_num\": 12,\n",
    "  \"layers_num\": 10,\n",
    "  \"dropout_rate\": 0.1,\n",
    "  \"use_bias\": False,\n",
    "}\n",
    "\n",
    "config[\"head_size\"] = config[\"embedding_dim\"] // config[\"heads_num\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.Q_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], config[\"use_bias\"])\n",
    "    self.K_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], config[\"use_bias\"])\n",
    "    self.V_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], config[\"use_bias\"])\n",
    "\n",
    "    self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
    "\n",
    "    casual_attention_mask = torch.tril(torch.ones(config[\"context_size\"], config[\"context_size\"]))\n",
    "    self.register_buffer('casual_attention_mask', casual_attention_mask)\n",
    "\n",
    "  def forward(self, input):\n",
    "    batch_size, tokens_num, embedding_dim = input.shape\n",
    "    Q = self.Q_weights(input)\n",
    "    K = self.K_weights(input)\n",
    "    V = self.V_weights(input)\n",
    "\n",
    "    attention_scores = Q @ K.transpose(1, 2)\n",
    "    attention_scores = attention_scores.masked_fill(\n",
    "        self.casual_attention_mask[:tokens_num,:tokens_num] == 0,\n",
    "        -torch.inf\n",
    "    )\n",
    "    attention_scores = attention_scores / ( K.shape[-1] ** 0.5 )\n",
    "    attention_scores = torch.softmax(attention_scores, dim=-1)\n",
    "    attention_scores = self.dropout(attention_scores)\n",
    "\n",
    "    return attention_scores @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ah = AttentionHead(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ah(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256, 64])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "\n",
    "    heads_list = [AttentionHead(config) for _ in range(config[\"heads_num\"])]\n",
    "    self.heads = nn.ModuleList(heads_list)\n",
    "\n",
    "    self.linear = nn.Linear(config[\"embedding_dim\"], config[\"embedding_dim\"])\n",
    "    self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
    "\n",
    "  def forward(self, input):\n",
    "    # print(f\"Input shape: {input.shape}\")\n",
    "    heads_outputs = [head(input) for head in self.heads]\n",
    "\n",
    "    scores_change = torch.cat(heads_outputs, dim=-1)\n",
    "    # print(f\"heads shape: {scores_change.shape}\")\n",
    "\n",
    "    scores_change = self.linear(scores_change)\n",
    "    return self.dropout(scores_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = mha(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "\n",
    "    self.linear_layers = nn.Sequential(\n",
    "        nn.Linear(config[\"embedding_dim\"], config[\"embedding_dim\"] * 4),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(config[\"embedding_dim\"] * 4, config[\"embedding_dim\"]),\n",
    "        nn.Dropout(config[\"dropout_rate\"])\n",
    "    )\n",
    "\n",
    "  def forward(self, input):\n",
    "    return self.linear_layers(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = FeedForward(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ouptut = ff(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256, 768])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "\n",
    "    self.multi_head = MultiHeadAttention(config)\n",
    "    self.layer_norm_1 = nn.LayerNorm(config[\"embedding_dim\"])\n",
    "\n",
    "    self.feed_forward = FeedForward(config)\n",
    "    self.layer_norm_2 = nn.LayerNorm(config[\"embedding_dim\"])\n",
    "\n",
    "  def forward(self, input):\n",
    "    residual = input\n",
    "    x = self.multi_head(self.layer_norm_1(input))\n",
    "    x = x + residual\n",
    "\n",
    "    residual = x\n",
    "    x = self.feed_forward(self.layer_norm_2(x))\n",
    "    return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Block(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ouptut = b(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256, 768])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoGPT(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    \n",
    "    # Create embedding vectors, which are as many as token ID in our vocabulary and have a dimension = 768.\n",
    "    # Add the positional encoding to have vectors for each position of the context window.\n",
    "    self.token_embedding_layer = nn.Embedding(config[\"vocabulary_size\"], config[\"embedding_dim\"])\n",
    "    self.positional_embedding_layer = nn.Embedding(config[\"context_size\"], config[\"embedding_dim\"])\n",
    "\n",
    "    # Create blocks one by one and combine them in a sequence.\n",
    "    blocks = [Block(config) for _ in range(config[\"layers_num\"])]\n",
    "    self.layers = nn.Sequential(*blocks)\n",
    "\n",
    "    # Normalization layer.\n",
    "    self.layer_norm = nn.LayerNorm(config[\"embedding_dim\"])\n",
    "    # Convert embedding vectors into prediction scores for what token ID should be generated.\n",
    "    self.unembedding = nn.Linear(config[\"embedding_dim\"], config[\"vocabulary_size\"], bias=False)\n",
    "\n",
    "  def forward(self, token_ids):\n",
    "    batch_size, tokens_num = token_ids.shape\n",
    "    \n",
    "    # Convert token IDs into embedding vectors and compute the positional encoding.\n",
    "    x = self.token_embedding_layer(token_ids)\n",
    "    sequence = torch.arange(tokens_num, device=device)\n",
    "    x = x + self.positional_embedding_layer(sequence)\n",
    "\n",
    "    x = self.layers(x)       # Apply the sequence of blocks.\n",
    "    x = self.layer_norm(x)   # Normalization.\n",
    "    x = self.unembedding(x)  # Embedding vectors -> logits.\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DemoGPT(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsqueeze method to add an extra dimention to our vector since the encode method returns a 1-D tensor and the model\n",
    "# expect a batch of inputs (2-D tensor).\n",
    "output = model(tokenizer.encode(\"Hi\").unsqueeze(dim=0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 98])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create text sequences using our model.a\n",
    "def generate(model, prompt_ids, max_tokens):\n",
    "    output_ids = prompt_ids\n",
    "    for _ in range(max_tokens):\n",
    "      if output_ids.shape[1] >= config[\"context_size\"]:\n",
    "        break\n",
    "      with torch.no_grad():\n",
    "        logits = model(output_ids)\n",
    "\n",
    "      logits = logits[:, -1, :]\n",
    "      probs = F.softmax(logits, dim=-1)\n",
    "      # Sample a random token given the softmax distribution\n",
    "      next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "      # Add new token to the output, and repeat the process\n",
    "      output_ids = torch.cat([output_ids, next_token_id], dim=-1)\n",
    "    return output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higher level wrapper for the \"generate\" function.\n",
    "def generate_with_prompt(model, tokenizer, prompt, max_tokens=100):\n",
    "  model.eval()\n",
    "\n",
    "  prompt = tokenizer.encode(prompt).unsqueeze(dim=0).to(device)\n",
    "\n",
    "  return tokenizer.decode(generate(model, prompt, max_tokens=max_tokens)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nëOéFZæ,jNuQDj‘qEx’…v3Oæ;BPx,(dW\\nC-YR“uMou[ëN 14’éà4çéaHCæàaE \\t\\n5jbpî;q-qR1YJ6æS“OX(æ7uqîç:CQOfà68vDv'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_with_prompt(model, tokenizer, \"First Citizen:\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The giberish at the output of \"generate_with_prompt()\" function is due to not training before the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
