{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Z2ea-5SaF59N"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Use CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9CRij9k1GDlM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "DSc0ludHGE1o"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "text = Path('tiny-shakespeare.txt').read_text(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7-9Nk7OoGGHc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    THE SONNETS\n",
      "    ALL’S WELL THAT ENDS WELL\n",
      "    THE TRAGEDY OF ANTONY AND CLEOPATRA\n",
      "    AS YOU LIKE IT\n",
      "    THE COMEDY OF ERRORS\n",
      "    THE TRAGEDY OF CORIOLANUS\n",
      "    CYMBELINE\n",
      "    THE TRAGEDY OF HAMLET, PRINCE OF DENMARK\n",
      "    THE FIRST PART OF KING HENRY THE FOURTH\n",
      "    THE SECOND PART OF KING HENRY THE FOURTH\n",
      "    THE LIFE OF KING HENRY THE FIFTH\n",
      "    THE FIRST PART OF HENRY THE SIXTH\n",
      "    THE SECOND PART OF KING HENRY THE SIXTH\n",
      "    THE THIRD PART OF KING HENRY THE SIXTH\n",
      "    KING HENRY THE EIGHTH\n",
      "    THE LIFE AND DEATH OF KING JOHN\n",
      "    THE TRAGEDY OF JULIUS CAESAR\n",
      "    THE TRAGEDY OF KING LEAR\n",
      "    LOVE’S LABOUR’S LOST\n",
      "    THE TRAGEDY OF MACBETH\n",
      "    MEASURE FOR MEASURE\n",
      "    THE MERCHANT OF VENICE\n",
      "    THE MERRY WIVES OF WINDSOR\n",
      "    A MIDSUMMER NIGHT’S DREAM\n",
      "    MUCH ADO ABOUT NOTHING\n",
      "    THE TRAGEDY OF OTHELLO, THE MOOR OF VENICE\n",
      "    PERICLES, PRINCE OF TYRE\n",
      "    KING RICHARD THE SECOND\n",
      "    KING RICHARD THE THIRD\n",
      "    THE TRAGEDY OF ROMEO AND JULIET\n",
      "    THE TAMING OF THE SHREW\n",
      "    THE TEMPEST\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(text[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "qg-yHMXPGHYq"
   },
   "outputs": [],
   "source": [
    "class CharTokenizer:\n",
    "  def __init__(self, vocabulary):\n",
    "    self.token_id_for_char = {char: token_id for token_id, char in enumerate(vocabulary)}\n",
    "    self.char_for_token_id = {token_id: char for token_id, char in enumerate(vocabulary)}\n",
    "\n",
    "  @staticmethod\n",
    "  def train_from_text(text):\n",
    "    vocabulary = set(text)\n",
    "    return CharTokenizer(sorted(list(vocabulary)))\n",
    "\n",
    "  def encode(self, text):\n",
    "    token_ids = []\n",
    "    for char in text:\n",
    "      token_ids.append(self.token_id_for_char[char])\n",
    "    return torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "  def decode(self, token_ids):\n",
    "    chars = []\n",
    "    for token_id in token_ids.tolist():\n",
    "      chars.append(self.char_for_token_id[token_id])\n",
    "    return ''.join(chars)\n",
    "\n",
    "  def vocabulary_size(self):\n",
    "    return len(self.token_id_for_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "pAYBFds4GNAb"
   },
   "outputs": [],
   "source": [
    "tokenizer = CharTokenizer.train_from_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "LKq-R9xvJ3RX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([31, 57, 64, 64, 67,  2, 75, 67, 70, 64, 56])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"Hello world\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "tWpR_hr9GOKs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(\"Hello world\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3a7qPM-mGPur"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocabulary_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "UbtX7_JtHFXL"
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "2I3m9KI6HM-S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\t',\n",
      " 1: '\\n',\n",
      " 2: ' ',\n",
      " 3: '!',\n",
      " 4: '&',\n",
      " 5: \"'\",\n",
      " 6: '(',\n",
      " 7: ')',\n",
      " 8: ',',\n",
      " 9: '-',\n",
      " 10: '.',\n",
      " 11: '0',\n",
      " 12: '1',\n",
      " 13: '2',\n",
      " 14: '3',\n",
      " 15: '4',\n",
      " 16: '5',\n",
      " 17: '6',\n",
      " 18: '7',\n",
      " 19: '8',\n",
      " 20: '9',\n",
      " 21: ':',\n",
      " 22: ';',\n",
      " 23: '?',\n",
      " 24: 'A',\n",
      " 25: 'B',\n",
      " 26: 'C',\n",
      " 27: 'D',\n",
      " 28: 'E',\n",
      " 29: 'F',\n",
      " 30: 'G',\n",
      " 31: 'H',\n",
      " 32: 'I',\n",
      " 33: 'J',\n",
      " 34: 'K',\n",
      " 35: 'L',\n",
      " 36: 'M',\n",
      " 37: 'N',\n",
      " 38: 'O',\n",
      " 39: 'P',\n",
      " 40: 'Q',\n",
      " 41: 'R',\n",
      " 42: 'S',\n",
      " 43: 'T',\n",
      " 44: 'U',\n",
      " 45: 'V',\n",
      " 46: 'W',\n",
      " 47: 'X',\n",
      " 48: 'Y',\n",
      " 49: 'Z',\n",
      " 50: '[',\n",
      " 51: ']',\n",
      " 52: '_',\n",
      " 53: 'a',\n",
      " 54: 'b',\n",
      " 55: 'c',\n",
      " 56: 'd',\n",
      " 57: 'e',\n",
      " 58: 'f',\n",
      " 59: 'g',\n",
      " 60: 'h',\n",
      " 61: 'i',\n",
      " 62: 'j',\n",
      " 63: 'k',\n",
      " 64: 'l',\n",
      " 65: 'm',\n",
      " 66: 'n',\n",
      " 67: 'o',\n",
      " 68: 'p',\n",
      " 69: 'q',\n",
      " 70: 'r',\n",
      " 71: 's',\n",
      " 72: 't',\n",
      " 73: 'u',\n",
      " 74: 'v',\n",
      " 75: 'w',\n",
      " 76: 'x',\n",
      " 77: 'y',\n",
      " 78: 'z',\n",
      " 79: 'Æ',\n",
      " 80: 'Ç',\n",
      " 81: 'É',\n",
      " 82: 'à',\n",
      " 83: 'â',\n",
      " 84: 'æ',\n",
      " 85: 'ç',\n",
      " 86: 'è',\n",
      " 87: 'é',\n",
      " 88: 'ê',\n",
      " 89: 'ë',\n",
      " 90: 'î',\n",
      " 91: 'œ',\n",
      " 92: '—',\n",
      " 93: '‘',\n",
      " 94: '’',\n",
      " 95: '“',\n",
      " 96: '”',\n",
      " 97: '…'}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(tokenizer.char_for_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "olX9rHjxHQih"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\t': 0,\n",
      " '\\n': 1,\n",
      " ' ': 2,\n",
      " '!': 3,\n",
      " '&': 4,\n",
      " \"'\": 5,\n",
      " '(': 6,\n",
      " ')': 7,\n",
      " ',': 8,\n",
      " '-': 9,\n",
      " '.': 10,\n",
      " '0': 11,\n",
      " '1': 12,\n",
      " '2': 13,\n",
      " '3': 14,\n",
      " '4': 15,\n",
      " '5': 16,\n",
      " '6': 17,\n",
      " '7': 18,\n",
      " '8': 19,\n",
      " '9': 20,\n",
      " ':': 21,\n",
      " ';': 22,\n",
      " '?': 23,\n",
      " 'A': 24,\n",
      " 'B': 25,\n",
      " 'C': 26,\n",
      " 'D': 27,\n",
      " 'E': 28,\n",
      " 'F': 29,\n",
      " 'G': 30,\n",
      " 'H': 31,\n",
      " 'I': 32,\n",
      " 'J': 33,\n",
      " 'K': 34,\n",
      " 'L': 35,\n",
      " 'M': 36,\n",
      " 'N': 37,\n",
      " 'O': 38,\n",
      " 'P': 39,\n",
      " 'Q': 40,\n",
      " 'R': 41,\n",
      " 'S': 42,\n",
      " 'T': 43,\n",
      " 'U': 44,\n",
      " 'V': 45,\n",
      " 'W': 46,\n",
      " 'X': 47,\n",
      " 'Y': 48,\n",
      " 'Z': 49,\n",
      " '[': 50,\n",
      " ']': 51,\n",
      " '_': 52,\n",
      " 'a': 53,\n",
      " 'b': 54,\n",
      " 'c': 55,\n",
      " 'd': 56,\n",
      " 'e': 57,\n",
      " 'f': 58,\n",
      " 'g': 59,\n",
      " 'h': 60,\n",
      " 'i': 61,\n",
      " 'j': 62,\n",
      " 'k': 63,\n",
      " 'l': 64,\n",
      " 'm': 65,\n",
      " 'n': 66,\n",
      " 'o': 67,\n",
      " 'p': 68,\n",
      " 'q': 69,\n",
      " 'r': 70,\n",
      " 's': 71,\n",
      " 't': 72,\n",
      " 'u': 73,\n",
      " 'v': 74,\n",
      " 'w': 75,\n",
      " 'x': 76,\n",
      " 'y': 77,\n",
      " 'z': 78,\n",
      " 'Æ': 79,\n",
      " 'Ç': 80,\n",
      " 'É': 81,\n",
      " 'à': 82,\n",
      " 'â': 83,\n",
      " 'æ': 84,\n",
      " 'ç': 85,\n",
      " 'è': 86,\n",
      " 'é': 87,\n",
      " 'ê': 88,\n",
      " 'ë': 89,\n",
      " 'î': 90,\n",
      " 'œ': 91,\n",
      " '—': 92,\n",
      " '‘': 93,\n",
      " '’': 94,\n",
      " '“': 95,\n",
      " '”': 96,\n",
      " '…': 97}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(tokenizer.token_id_for_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NLnPsybaHTew"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
