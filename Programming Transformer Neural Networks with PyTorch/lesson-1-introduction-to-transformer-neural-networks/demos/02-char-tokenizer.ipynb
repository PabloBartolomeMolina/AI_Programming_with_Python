{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2ea-5SaF59N"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Use CUDA if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "9CRij9k1GDlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "text = Path('tiny-shakespeare.txt').read_text()"
      ],
      "metadata": {
        "id": "DSc0ludHGE1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[0:1000])"
      ],
      "metadata": {
        "id": "7-9Nk7OoGGHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharTokenizer:\n",
        "  def __init__(self, vocabulary):\n",
        "    self.token_id_for_char = {char: token_id for token_id, char in enumerate(vocabulary)}\n",
        "    self.char_for_token_id = {token_id: char for token_id, char in enumerate(vocabulary)}\n",
        "\n",
        "  @staticmethod\n",
        "  def train_from_text(text):\n",
        "    vocabulary = set(text)\n",
        "    return CharTokenizer(sorted(list(vocabulary)))\n",
        "\n",
        "  def encode(self, text):\n",
        "    token_ids = []\n",
        "    for char in text:\n",
        "      token_ids.append(self.token_id_for_char[char])\n",
        "    return torch.tensor(token_ids, dtype=torch.long)\n",
        "\n",
        "  def decode(self, token_ids):\n",
        "    chars = []\n",
        "    for token_id in token_ids.tolist():\n",
        "      chars.append(self.char_for_token_id[token_id])\n",
        "    return ''.join(chars)\n",
        "\n",
        "  def vocabulary_size(self):\n",
        "    return len(self.token_id_for_char)"
      ],
      "metadata": {
        "id": "qg-yHMXPGHYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = CharTokenizer.train_from_text(text)"
      ],
      "metadata": {
        "id": "pAYBFds4GNAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.encode(\"Hello world\"))"
      ],
      "metadata": {
        "id": "LKq-R9xvJ3RX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(tokenizer.encode(\"Hello world\")))"
      ],
      "metadata": {
        "id": "tWpR_hr9GOKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.vocabulary_size()"
      ],
      "metadata": {
        "id": "3a7qPM-mGPur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "pp = pprint.PrettyPrinter(depth=4)"
      ],
      "metadata": {
        "id": "UbtX7_JtHFXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pp.pprint(tokenizer.char_for_token_id)"
      ],
      "metadata": {
        "id": "2I3m9KI6HM-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pp.pprint(tokenizer.token_id_for_char)"
      ],
      "metadata": {
        "id": "olX9rHjxHQih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NLnPsybaHTew"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}