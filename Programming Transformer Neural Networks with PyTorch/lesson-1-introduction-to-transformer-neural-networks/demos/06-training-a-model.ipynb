{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CMWMOaDg-ZBv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Use CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "text = Path('tiny-shakespeare.txt').read_text(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IwwOe-tJ-xcE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    THE SONNETS\n",
      "    ALL’S WELL THAT ENDS WELL\n",
      "    THE TRAGEDY OF ANTONY AND CLEOPATRA\n",
      "    AS YOU LIKE IT\n",
      "    THE COMEDY OF ERRORS\n",
      "    THE TRAGEDY OF CORIOLANUS\n",
      "    CYMBELINE\n",
      "    THE TRAGEDY OF HAMLET, PRINCE OF DENMARK\n",
      "    THE FIRST PART OF KING HENRY THE FOURTH\n",
      "    THE SECOND PART OF KING HENRY THE FOURTH\n",
      "    THE LIFE OF KING HENRY THE FIFTH\n",
      "    THE FIRST PART OF HENRY THE SIXTH\n",
      "    THE SECOND PART OF KING HENRY THE SIXTH\n",
      "    THE THIRD PART OF KING HENRY THE SIXTH\n",
      "    KING HENRY THE EIGHTH\n",
      "    THE LIFE AND DEATH OF KING JOHN\n",
      "    THE TRAGEDY OF JULIUS CAESAR\n",
      "    THE TRAGEDY OF KING LEAR\n",
      "    LOVE’S LABOUR’S LOST\n",
      "    THE TRAGEDY OF MACBETH\n",
      "    MEASURE FOR MEASURE\n",
      "    THE MERCHANT OF VENICE\n",
      "    THE MERRY WIVES OF WINDSOR\n",
      "    A MIDSUMMER NIGHT’S DREAM\n",
      "    MUCH ADO ABOUT NOTHING\n",
      "    THE TRAGEDY OF OTHELLO, THE MOOR OF VENICE\n",
      "    PERICLES, PRINCE OF TYRE\n",
      "    KING RICHARD THE SECOND\n",
      "    KING RICHARD THE THIRD\n",
      "    THE TRAGEDY OF ROMEO AND JULIET\n",
      "    THE TAMING OF THE SHREW\n",
      "    THE TEMPEST\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(text[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ap_Ixr0M-0Yv"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CharTokenizer:\n",
    "  def __init__(self, vocabulary):\n",
    "    self.token_id_for_char = {char: token_id for token_id, char in enumerate(vocabulary)}\n",
    "    self.char_for_token_id = {token_id: char for token_id, char in enumerate(vocabulary)}\n",
    "\n",
    "  @staticmethod\n",
    "  def train_from_text(text):\n",
    "    vocabulary = set(text)\n",
    "    return CharTokenizer(sorted(list(vocabulary)))\n",
    "\n",
    "  def encode(self, text):\n",
    "    token_ids = []\n",
    "    for char in text:\n",
    "      token_ids.append(self.token_id_for_char[char])\n",
    "    return torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "  def decode(self, token_ids):\n",
    "    chars = []\n",
    "    for token_id in token_ids.tolist():\n",
    "      chars.append(self.char_for_token_id[token_id])\n",
    "    return ''.join(chars)\n",
    "\n",
    "\n",
    "  def vocabulary_size(self):\n",
    "    return len(self.token_id_for_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "T3q9Dj3l-2Ja"
   },
   "outputs": [],
   "source": [
    "tokenizer = CharTokenizer.train_from_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Lb1zImZr-4mY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([31, 57, 64, 64, 67,  2, 75, 67, 70, 64, 56])\n",
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"Hello world\"))\n",
    "print(tokenizer.decode(tokenizer.encode(\"Hello world\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MlTEiIqs-7Uz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 98\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size: {tokenizer.vocabulary_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7Qal76ig-94U"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TokenIdsDataset(Dataset):\n",
    "  def __init__(self, data, block_size):\n",
    "    self.data = data\n",
    "    self.block_size = block_size\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data) - self.block_size\n",
    "\n",
    "  def __getitem__(self, pos):\n",
    "    assert pos < len(self.data) - self.block_size\n",
    "\n",
    "    x = self.data[pos:pos + self.block_size]\n",
    "    y = self.data[pos + 1:pos + 1 + self.block_size]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"vocabulary_size\": tokenizer.vocabulary_size(),\n",
    "  \"context_size\": 256,\n",
    "  \"embedding_dim\": 768,\n",
    "  \"heads_num\": 12,\n",
    "  \"layers_num\": 10,\n",
    "  \"dropout_rate\": 0.1,\n",
    "  \"use_bias\": False,\n",
    "}\n",
    "\n",
    "config[\"head_size\"] = config[\"embedding_dim\"] // config[\"heads_num\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.Q_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], config[\"use_bias\"])\n",
    "    self.K_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], config[\"use_bias\"])\n",
    "    self.V_weights = nn.Linear(config[\"embedding_dim\"], config[\"head_size\"], config[\"use_bias\"])\n",
    "\n",
    "    self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
    "\n",
    "    casual_attention_mask = torch.tril(torch.ones(config[\"context_size\"], config[\"context_size\"]))\n",
    "    self.register_buffer('casual_attention_mask', casual_attention_mask)\n",
    "\n",
    "  def forward(self, input):\n",
    "    batch_size, tokens_num, embedding_dim = input.shape\n",
    "    Q = self.Q_weights(input)\n",
    "    K = self.K_weights(input)\n",
    "    V = self.V_weights(input)\n",
    "\n",
    "    attention_scores = Q @ K.transpose(1, 2)\n",
    "    attention_scores = attention_scores.masked_fill(\n",
    "        self.casual_attention_mask[:tokens_num,:tokens_num] == 0,\n",
    "        -torch.inf\n",
    "    )\n",
    "    attention_scores = attention_scores / ( K.shape[-1] ** 0.5 )\n",
    "    attention_scores = torch.softmax(attention_scores, dim=-1)\n",
    "    attention_scores = self.dropout(attention_scores)\n",
    "\n",
    "    return attention_scores @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ah = AttentionHead(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ah(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256, 64])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "\n",
    "    heads_list = [AttentionHead(config) for _ in range(config[\"heads_num\"])]\n",
    "    self.heads = nn.ModuleList(heads_list)\n",
    "\n",
    "    self.linear = nn.Linear(config[\"embedding_dim\"], config[\"embedding_dim\"])\n",
    "    self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
    "\n",
    "  def forward(self, input):\n",
    "    # print(f\"Input shape: {input.shape}\")\n",
    "    heads_outputs = [head(input) for head in self.heads]\n",
    "\n",
    "    scores_change = torch.cat(heads_outputs, dim=-1)\n",
    "    # print(f\"heads shape: {scores_change.shape}\")\n",
    "\n",
    "    scores_change = self.linear(scores_change)\n",
    "    return self.dropout(scores_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = mha(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "\n",
    "    self.linear_layers = nn.Sequential(\n",
    "        nn.Linear(config[\"embedding_dim\"], config[\"embedding_dim\"] * 4),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(config[\"embedding_dim\"] * 4, config[\"embedding_dim\"]),\n",
    "        nn.Dropout(config[\"dropout_rate\"])\n",
    "    )\n",
    "\n",
    "  def forward(self, input):\n",
    "    return self.linear_layers(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = FeedForward(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand(8, config[\"context_size\"], config[\"embedding_dim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ouptut = ff(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256, 768])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "\n",
    "    self.multi_head = MultiHeadAttention(config)\n",
    "    self.layer_norm_1 = nn.LayerNorm(config[\"embedding_dim\"])\n",
    "\n",
    "    self.feed_forward = FeedForward(config)\n",
    "    self.layer_norm_2 = nn.LayerNorm(config[\"embedding_dim\"])\n",
    "\n",
    "  def forward(self, input):\n",
    "    residual = input\n",
    "    x = self.multi_head(self.layer_norm_1(input))\n",
    "    x = x + residual\n",
    "\n",
    "    residual = x\n",
    "    x = self.feed_forward(self.layer_norm_2(x))\n",
    "    return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Block(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ouptut = b(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256, 768])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoGPT(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "\n",
    "    self.token_embedding_layer = nn.Embedding(config[\"vocabulary_size\"], config[\"embedding_dim\"])\n",
    "    self.positional_embedding_layer = nn.Embedding(config[\"context_size\"], config[\"embedding_dim\"])\n",
    "\n",
    "    blocks = [Block(config) for _ in range(config[\"layers_num\"])]\n",
    "    self.layers = nn.Sequential(*blocks)\n",
    "\n",
    "    self.layer_norm = nn.LayerNorm(config[\"embedding_dim\"])\n",
    "    self.unembedding = nn.Linear(config[\"embedding_dim\"], config[\"vocabulary_size\"], bias=False)\n",
    "\n",
    "  def forward(self, token_ids):\n",
    "    # print(\"Forward\")\n",
    "    batch_size, tokens_num = token_ids.shape\n",
    "\n",
    "    x = self.token_embedding_layer(token_ids)\n",
    "    sequence = torch.arange(tokens_num, device=device)\n",
    "    x = x + self.positional_embedding_layer(sequence)\n",
    "\n",
    "    x = self.layers(x)\n",
    "    x = self.layer_norm(x)\n",
    "    x = self.unembedding(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DemoGPT(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(tokenizer.encode(\"Hi\").unsqueeze(dim=0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 98])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, prompt_ids, max_tokens):\n",
    "    output_ids = prompt_ids\n",
    "    for _ in range(max_tokens):\n",
    "      if output_ids.shape[1] >= config[\"context_size\"]:\n",
    "        break\n",
    "      with torch.no_grad():\n",
    "        logits = model(output_ids)\n",
    "\n",
    "      logits = logits[:, -1, :]\n",
    "      probs = F.softmax(logits, dim=-1)\n",
    "      # Sample a random token given the softmax distribution\n",
    "      next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "      # Add new token to the output, and repeat the process\n",
    "      output_ids = torch.cat([output_ids, next_token_id], dim=-1)\n",
    "    return output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_prompt(model, tokenizer, prompt, max_tokens=100):\n",
    "  model.eval()\n",
    "\n",
    "  prompt = tokenizer.encode(prompt).unsqueeze(dim=0).to(device)\n",
    "\n",
    "  return tokenizer.decode(generate(model, prompt, max_tokens=max_tokens)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\n‘fzve\\tMi_z&7XæC_f3\\n6dReà8y1w(5ÉrVAkg[F[Slry(ejHmSÆx”U4æHQ685INj6K6XO[?æ”dqèukLpàQrPv—ÇS,fv[f?U2taF.œ'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_with_prompt(model, tokenizer, \"First Citizen:\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_iterations = 5000\n",
    "evaluation_interval = 100\n",
    "learning_rate=4e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tokenizer.encode(text).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TokenIdsDataset(train_data, config[\"context_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset, num_samples=batch_size * train_iterations, replacement=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.72 GiB is allocated by PyTorch, and 80.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28minput\u001b[39m, targets \u001b[38;5;241m=\u001b[39m sample\n\u001b[1;32m----> 5\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m      7\u001b[0m logits_view \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mview(batch_size \u001b[38;5;241m*\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext_size\u001b[39m\u001b[38;5;124m\"\u001b[39m], config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocabulary_size\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      8\u001b[0m targets_view \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mview(batch_size \u001b[38;5;241m*\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext_size\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[29], line 18\u001b[0m, in \u001b[0;36mDemoGPT.forward\u001b[1;34m(self, token_ids)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, token_ids):\n\u001b[0;32m     15\u001b[0m   \u001b[38;5;66;03m# print(\"Forward\")\u001b[39;00m\n\u001b[0;32m     16\u001b[0m   batch_size, tokens_num \u001b[38;5;241m=\u001b[39m token_ids\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m---> 18\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding_layer(token_ids)\n\u001b[0;32m     19\u001b[0m   sequence \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(tokens_num, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     20\u001b[0m   x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding_layer(sequence)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m    193\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx,\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type,\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq,\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse,\n\u001b[0;32m    198\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.72 GiB is allocated by PyTorch, and 80.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "for step_num, sample in enumerate(train_dataloader):\n",
    "\n",
    "  model.train()\n",
    "  input, targets = sample\n",
    "  logits = model(input)\n",
    "\n",
    "  logits_view = logits.view(batch_size * config[\"context_size\"], config[\"vocabulary_size\"])\n",
    "  targets_view = targets.view(batch_size * config[\"context_size\"])\n",
    "  \n",
    "  loss = F.cross_entropy(logits_view, targets_view)\n",
    "  # Backward propagation\n",
    "  loss.backward()\n",
    "  # Update model parameters\n",
    "  optimizer.step()\n",
    "  # Set to None to reduce memory usage\n",
    "  optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "  print(f\"Step {step_num}. Loss {loss.item():.3f}\")\n",
    "\n",
    "  if step_num % evaluation_interval == 0:\n",
    "    print(\"Demo GPT:\\n\" + generate_with_prompt(model, tokenizer, \"\\n\"))\n",
    "  # Clean memory to optimize use of ressources from the GPU.\n",
    "  del input, targets, logits, logits_view, targets_view, loss\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
