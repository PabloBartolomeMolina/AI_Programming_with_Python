{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ulgnkbbvEt6u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "cuda_is_available = torch.cuda.is_available()\n",
    "\n",
    "if cuda_is_available:\n",
    "  print(\"All good!\")\n",
    "else:\n",
    "  print(\"CUDA is NOT available!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7R6RnqQu0-P9"
   },
   "outputs": [],
   "source": [
    "prompt = \"During the latest presentation OpenAI\"\n",
    "model = \"openai-community/gpt2-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hkVKzDegAItu"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba35aecf50dc4f9e8b84b1a748db2a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the text generation pipeline\n",
    "text_generator = pipeline(\"text-generation\", model=model, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xUAq3XiAN1z"
   },
   "outputs": [],
   "source": [
    "# Generate text\n",
    "generated_texts = text_generator(prompt, max_length=100, num_return_sequences=1)\n",
    "\n",
    "print(f\"Generated text:\\n{generated_texts[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGcDDPcz1Ln5"
   },
   "outputs": [],
   "source": [
    "for do_sample in [\n",
    "    False, # Greedy Search\n",
    "    True   # Multinomial sampling\n",
    "  ]:\n",
    "  generated_texts = text_generator(prompt, max_length=100, num_return_sequences=1, do_sample=do_sample, num_beams=1)\n",
    "\n",
    "  print(\"-----------------------------------\")\n",
    "  print(\"Parameters:\")\n",
    "  print(\"-----------------------------------\")\n",
    "  print(f\"do_sample={do_sample}\")\n",
    "  print(\"-----------------------------------\")\n",
    "  print(\"Generation:\")\n",
    "  print(\"-----------------------------------\")\n",
    "  print(generated_texts[0]['generated_text'])\n",
    "  print(\"-----------------------------------\")\n",
    "  print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uo3CC3yk4QmC"
   },
   "outputs": [],
   "source": [
    "# Beam-search strategy\n",
    "\n",
    "for beams in [1, 2, 4, 8]:\n",
    "  generated_texts = text_generator(prompt, max_length=100, num_return_sequences=1, do_sample=False, num_beams=beams)\n",
    "\n",
    "  print(\"-----------------------------------\")\n",
    "  print(\"Parameters:\")\n",
    "  print(\"-----------------------------------\")\n",
    "  print(f\"num_beams={beams}\")\n",
    "  print(\"-----------------------------------\")\n",
    "  print(\"Generation:\")\n",
    "  print(\"-----------------------------------\")\n",
    "  print(generated_texts[0]['generated_text'])\n",
    "  print(\"-----------------------------------\")\n",
    "  print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K96hVRPC6MUR"
   },
   "outputs": [],
   "source": [
    "# Beam-search multinomial sampling\n",
    "\n",
    "for beams in [1, 2, 4, 8]:\n",
    "  generated_texts = text_generator(prompt, max_length=100, num_return_sequences=1, do_sample=True, num_beams=beams)\n",
    "\n",
    "  print(\"-----------------------------------\")\n",
    "  print(\"Parameters:\")\n",
    "  print(\"-----------------------------------\")\n",
    "  print(f\"num_beams={beams}\")\n",
    "  print(\"-----------------------------------\")\n",
    "  print(\"Generation:\")\n",
    "  print(\"-----------------------------------\")\n",
    "  print(generated_texts[0]['generated_text'])\n",
    "  print(\"-----------------------------------\")\n",
    "  print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f5R7GeD3AUgp"
   },
   "outputs": [],
   "source": [
    "# Change the top-k parameter\n",
    "\n",
    "for k in [1, 5, 10, 50, 100, 500]:\n",
    "  generated_texts = text_generator(prompt, max_length=100, num_return_sequences=1, top_k=k)\n",
    "\n",
    "  print(\"-----------------------------------\")\n",
    "  print(\"Parameters:\")\n",
    "  print(\"-----------------------------------\")\n",
    "  print(f\"top_k={k}\")\n",
    "  print(\"-----------------------------------\")\n",
    "  print(\"Generation:\")\n",
    "  print(\"-----------------------------------\")\n",
    "  print(generated_texts[0]['generated_text'])\n",
    "  print(\"-----------------------------------\")\n",
    "  print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yXOuaxe0RJwf"
   },
   "outputs": [],
   "source": [
    "# Change temperature\n",
    "\n",
    "for temp in [0.1, 1.0, 2.0, 3.0]:\n",
    "  generated_texts = text_generator(prompt, max_length=100, num_return_sequences=1, temperature=temp)\n",
    "\n",
    "  print(\"-----------------------------------\")\n",
    "  print(\"Parameters:\")\n",
    "  print(\"-----------------------------------\")\n",
    "  print(f\"temperature={temp}\")\n",
    "  print(\"-----------------------------------\")\n",
    "  print(\"Generation:\")\n",
    "  print(\"-----------------------------------\")\n",
    "  print(generated_texts[0]['generated_text'])\n",
    "  print(\"-----------------------------------\")\n",
    "  print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUiu5ZdT9ArW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
