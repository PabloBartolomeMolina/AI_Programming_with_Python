{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CMWMOaDg-ZBv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Use CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "text = Path('../../tiny-shakespeare.txt').read_text(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IwwOe-tJ-xcE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    THE SONNETS\n",
      "    ALL’S WELL THAT ENDS WELL\n",
      "    THE TRAGEDY OF ANTONY AND CLEOPATRA\n",
      "    AS YOU LIKE IT\n",
      "    THE COMEDY OF ERRORS\n",
      "    THE TRAGEDY OF CORIOLANUS\n",
      "    CYMBELINE\n",
      "    THE TRAGEDY OF HAMLET, PRINCE OF DENMARK\n",
      "    THE FIRST PART OF KING HENRY THE FOURTH\n",
      "    THE SECOND PART OF KING HENRY THE FOURTH\n",
      "    THE LIFE OF KING HENRY THE FIFTH\n",
      "    THE FIRST PART OF HENRY THE SIXTH\n",
      "    THE SECOND PART OF KING HENRY THE SIXTH\n",
      "    THE THIRD PART OF KING HENRY THE SIXTH\n",
      "    KING HENRY THE EIGHTH\n",
      "    THE LIFE AND DEATH OF KING JOHN\n",
      "    THE TRAGEDY OF JULIUS CAESAR\n",
      "    THE TRAGEDY OF KING LEAR\n",
      "    LOVE’S LABOUR’S LOST\n",
      "    THE TRAGEDY OF MACBETH\n",
      "    MEASURE FOR MEASURE\n",
      "    THE MERCHANT OF VENICE\n",
      "    THE MERRY WIVES OF WINDSOR\n",
      "    A MIDSUMMER NIGHT’S DREAM\n",
      "    MUCH ADO ABOUT NOTHING\n",
      "    THE TRAGEDY OF OTHELLO, THE MOOR OF VENICE\n",
      "    PERICLES, PRINCE OF TYRE\n",
      "    KING RICHARD THE SECOND\n",
      "    KING RICHARD THE THIRD\n",
      "    THE TRAGEDY OF ROMEO AND JULIET\n",
      "    THE TAMING OF THE SHREW\n",
      "    THE TEMPEST\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(text[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Ap_Ixr0M-0Yv"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CharTokenizer:\n",
    "  def __init__(self, vocabulary):\n",
    "    self.token_id_for_char = {char: token_id for token_id, char in enumerate(vocabulary)}\n",
    "    self.char_for_token_id = {token_id: char for token_id, char in enumerate(vocabulary)}\n",
    "\n",
    "  @staticmethod\n",
    "  def train_from_text(text):\n",
    "    vocabulary = set(text)\n",
    "    return CharTokenizer(sorted(list(vocabulary)))\n",
    "\n",
    "  def encode(self, text):\n",
    "    token_ids = []\n",
    "    for char in text:\n",
    "      token_ids.append(self.token_id_for_char[char])\n",
    "    return torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "  def decode(self, token_ids):\n",
    "    chars = []\n",
    "    for token_id in token_ids.tolist():\n",
    "      chars.append(self.char_for_token_id[token_id])\n",
    "    return ''.join(chars)\n",
    "\n",
    "\n",
    "  def vocabulary_size(self):\n",
    "    return len(self.token_id_for_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "T3q9Dj3l-2Ja"
   },
   "outputs": [],
   "source": [
    "tokenizer = CharTokenizer.train_from_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Lb1zImZr-4mY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([31, 57, 64, 64, 67,  2, 75, 67, 70, 64, 56])\n",
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"Hello world\"))\n",
    "print(tokenizer.decode(tokenizer.encode(\"Hello world\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "MlTEiIqs-7Uz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 98\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size: {tokenizer.vocabulary_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "7Qal76ig-94U"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TokenIdsDataset(Dataset):\n",
    "  def __init__(self, data, block_size):\n",
    "    self.data = data\n",
    "    self.block_size = block_size\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data) - self.block_size\n",
    "\n",
    "  def __getitem__(self, pos):\n",
    "    assert pos < len(self.data) - self.block_size\n",
    "\n",
    "    x = self.data[pos:pos + self.block_size]\n",
    "    y = self.data[pos + 1:pos + 1 + self.block_size]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "oPPfQ5n1_BuV"
   },
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer.encode(text)\n",
    "dataset = TokenIdsDataset(tokenized_text, block_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  2,  2,  2, 43, 31, 28,  2, 42, 38, 37, 37, 28, 43, 42,  1,  2,  2,\n",
       "         2,  2, 24, 35, 35, 94, 42,  2, 46, 28, 35, 35,  2, 43, 31, 24, 43,  2,\n",
       "        28, 37, 27, 42,  2, 46, 28, 35, 35,  1,  2,  2,  2,  2, 43, 31, 28,  2,\n",
       "        43, 41, 24, 30, 28, 27, 48,  2, 38, 29])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    THE SONNETS\\n    ALL’S WELL THAT ENDS WELL\\n    THE TRAGEDY OF'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "pU5xPNPN_RSQ"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "sampler = RandomSampler(dataset, replacement=True)\n",
    "dataloader = DataLoader(dataset, batch_size=2, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "cmoIkxKP_gBD"
   },
   "outputs": [],
   "source": [
    "x, y = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "oKvdaDFb_mxv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2, 53,  2, 65, 67, 66, 71, 72, 57, 70, 10,  1,  1, 38, 35, 27,  2, 35,\n",
       "         24, 27, 48, 10,  1, 31, 57, 53, 70, 72, 71,  2, 67, 58,  2, 65, 67, 71,\n",
       "         72,  2, 60, 53, 70, 56,  2, 72, 57, 65, 68, 57, 70,  1, 36, 57, 64, 72,\n",
       "          2, 53, 66, 56,  2, 64, 53, 65, 57, 66],\n",
       "        [70, 23,  2, 48, 67, 73,  2, 56, 67,  2, 71, 73, 70, 57, 64, 77,  2, 54,\n",
       "         53, 70,  2, 72, 60, 57,  1, 56, 67, 67, 70,  2, 73, 68, 67, 66,  2, 77,\n",
       "         67, 73, 70,  2, 67, 75, 66,  2, 64, 61, 54, 57, 70, 72, 77,  2, 61, 58,\n",
       "          2, 77, 67, 73,  2, 56, 57, 66, 77,  2]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "mRYTdaFs_nrH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' a monster.\\n\\nOLD LADY.\\nHearts of most hard temper\\nMelt and lamen'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "7z-wrmxL_ucH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a monster.\\n\\nOLD LADY.\\nHearts of most hard temper\\nMelt and lament'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
