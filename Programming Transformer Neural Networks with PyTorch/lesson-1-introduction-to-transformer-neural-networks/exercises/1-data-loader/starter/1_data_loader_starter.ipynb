{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMWMOaDg-ZBv"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "text = Path('tiny-shakespeare.txt').read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IwwOe-tJ-xcE"
   },
   "outputs": [],
   "source": [
    "print(text[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ap_Ixr0M-0Yv"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CharTokenizer:\n",
    "  def __init__(self, vocabulary):\n",
    "    self.token_id_for_char = {char: token_id for token_id, char in enumerate(vocabulary)}\n",
    "    self.char_for_token_id = {token_id: char for token_id, char in enumerate(vocabulary)}\n",
    "\n",
    "  @staticmethod\n",
    "  def train_from_text(text):\n",
    "    vocabulary = set(text)\n",
    "    return CharTokenizer(sorted(list(vocabulary)))\n",
    "\n",
    "  def encode(self, text):\n",
    "    token_ids = []\n",
    "    for char in text:\n",
    "      token_ids.append(self.token_id_for_char[char])\n",
    "    return torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "  def decode(self, token_ids):\n",
    "    chars = []\n",
    "    for token_id in token_ids.tolist():\n",
    "      chars.append(self.char_for_token_id[token_id])\n",
    "    return ''.join(chars)\n",
    "\n",
    "\n",
    "  def vocabulary_size(self):\n",
    "    return len(self.token_id_for_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3q9Dj3l-2Ja"
   },
   "outputs": [],
   "source": [
    "tokenizer = CharTokenizer.train_from_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lb1zImZr-4mY"
   },
   "outputs": [],
   "source": [
    "print(tokenizer.encode(\"Hello world\"))\n",
    "print(tokenizer.decode(tokenizer.encode(\"Hello world\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MlTEiIqs-7Uz"
   },
   "outputs": [],
   "source": [
    "print(f\"Vocabulary size: {tokenizer.vocabulary_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Qal76ig-94U"
   },
   "outputs": [],
   "source": [
    "# Step 1 - Define the `TokenIdsDataset` Class\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TokenIdsDataset(Dataset):\n",
    "  def __init__(self, data, block_size):\n",
    "    # TODO: Save data and block size\n",
    "\n",
    "  def __len__(self):\n",
    "    # TODO: If every position can be a start of an item,\n",
    "    # and all items should be \"block_size\", compute the size\n",
    "    # of the dataset\n",
    "\n",
    "  def __getitem__(self, pos):\n",
    "    # TODO: Check if the input position is valid\n",
    "\n",
    "    # TODO: Get an item from position \"pos\"\n",
    "    # TODO: Get a target item (shifted by one position)\n",
    "\n",
    "    # TODO: Return both\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oPPfQ5n1_BuV"
   },
   "outputs": [],
   "source": [
    "# Step 2 - Tokenize the Text\n",
    "\n",
    "# TODO: Encode text using the tokenizer\n",
    "# Create \"TokenIdsDataset\" with the tokenized text, and block_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Retrieve the First Item from the Dataset\n",
    "\n",
    "# TODO: Get the first item from the dataset\n",
    "# Decode \"x\" using tokenizer.decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pU5xPNPN_RSQ"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "# RandomSampler allows to read random items from a datasset\n",
    "sampler = RandomSampler(dataset, replacement=True)\n",
    "# Dataloader will laod two random samplers using the sampler\n",
    "dataloader = DataLoader(dataset, batch_size=2, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cmoIkxKP_gBD"
   },
   "outputs": [],
   "source": [
    "# Step 4 - Use a DataLoader\n",
    "\n",
    "# TODO: Get a single batch from the \"dataloader\"\n",
    "# For this call the `iter` function, and pass DataLoader instance to it. This will create an iterator\n",
    "# Then call the `next` function and pass the iterator to it to get the first training batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mRYTdaFs_nrH"
   },
   "outputs": [],
   "source": [
    "# TODO: Decode input item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7z-wrmxL_ucH"
   },
   "outputs": [],
   "source": [
    "# TODO: Decode target item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZyqnpXW7_w__"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
